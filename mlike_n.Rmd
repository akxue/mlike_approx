---
title: "Log Marginal Likelihood vs. Sample Size"
output:
    prettydoc::html_pretty:
         toc: true
    theme: cayman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE, message = FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)

# change path depending on Dell or Lenovo
setwd("C:/Users/chuu/mlike_approx")
```

## 2-d Singular Example

Setup: $u \in \left[ 0, 1 \right]^2$, $K(u) = u_1^2 u_2^4$. 

Density: $\gamma_n (u) = \frac{1}{\mathcal{Z}} \cdot e^{-n K(u)} \cdot \pi(u)$, where $\pi(\cdot)$ is the uniform measure on $\left[ 0, 1 \right]^2$. 

We perform the following steps:

1. Use STAN to draw samples from $\gamma_n$ for a given $n$
2. Use these samples to estimate $\mathcal{Z}_n$ using your method. 
3. Repeat (1) and (2) for a range of values of n. 
4. Regress your (estimate for) $\log \mathcal{Z}_n$ on $\log n$ and check if the slope parameter equals -0.25. 

Simulation results below show a slope of -0.3562. 

Details: For each $n$, we averaged $200$ approximations, with each approximation using $J = 2000$ samples from the posterior. 

```{r, echo = FALSE}

approx_N = read.csv("singular/singular_lil_v1.csv")

approx_mean = colMeans(approx_N)

N_vec  = seq(100, 10000, 200)

z_n = exp(approx_mean)
log_z_n = approx_mean
log_n = log(N_vec)

lil_df = data.frame(z_n, log_z_n, log_n)

ggplot(lil_df, aes(log_n, log_z_n)) + geom_point()

lm(log_z_n ~ log_n, lil_df) 

# intercept = 10.9585
# slope     = -0.3562
```



## 1-d Normal-Normal Example (unknown mean, known variance)

Setup: $X_1, \ldots, X_N \sim N(\mu, \sigma^2)$, $\mu \sim N\left( m_0, \tau^2 \right)$ 

Then, for $N = 50, 100, 200, 500, 1000, 10000$, we compute the marginal likelihood. 

```{r, echo = FALSE}
set.seed(1)
mu = 30
sigmasq = 4
m_0 = 0 
tau = 3

N_grid = c(50, 100, 200, 500, 1000, 10000)
iters = length(N_grid)

lil = numeric(iters) # store log marginal likelihood for each n

n_sims = 200 # number of times to compute LIL for EACH value of N

for (i in 1:iters) {
    
    N = N_grid[i]
    
    for (j in 1:n_sims) {
        x = rnorm(N, mean = mu, sd = sqrt(sigmasq))
        xbar = mean(x)
        
        m_0 = 0 
        tau = 3
        
        lil[i] = log(sqrt(sigmasq)) - N * log(2 * pi * sqrt(sigmasq)) - 
            0.5 * log(N * tau^2 + sigmasq) - sum(x^2) / (2 * sigmasq) - 
            m_0^2 / (2 * tau^2) + 1 / (2 * (N * tau^2 + sigmasq)) * 
            (tau^2 * N^2 * xbar^2 / sigmasq + sigmasq * m_0^2 / tau^2 + 
                 2 * N * xbar * m_0)
    }
    
}

lil_df = data.frame(n = N_grid, lil = lil)
```

### log marginal likelihood vs. log n

```{r, echo = FALSE}
ggplot(lil_df, aes(log(n), lil)) + geom_point() + theme_bw()

lm(lil ~ log(n), lil_df)
```



## 2-d Normal-Inverse Gamma Example

```{r, echo = FALSE}
set.seed(1)
mu = 30
sigma_sq = 4
m_0 = 0
w_0 = 0.05
r_0 = 3
s_0 = 3

N_grid = c(50, 100, 200, 500, 1000, 10000)


#### generate data
LIL_n = numeric(length(N_grid))

for (n in 1:length(N_grid)) {
    
    N = N_grid[n]
    
    y = rnorm(N, mu, sqrt(sigma_sq))
    
    #### compute posterior parameters
    ybar = mean(y)
    m_n = (N * ybar + w_0 * m_0) / (N + w_0)
    w_n = w_0 + N
    r_n = r_0 + N
    s_n = s_0 + sum((y - ybar)^2) + (N * w_0 / (N + w_0)) * (ybar - m_0)^2
    
    
    #### compute true (log) marginal likelihood
    
    LIL_n[n] = -(N/2) * log(pi) + 0.5 * (log(w_0) - log(w_n)) + 
        lgamma(r_n / 2) - lgamma(r_0 / 2) + r_0 / 2 * log(s_0) - 
        r_n / 2 * log(s_n)
}


lil_df = data.frame(n = N_grid, lil = LIL_n)
```


### log marginal likelihood vs. log n

```{r, echo = FALSE}
ggplot(lil_df, aes(log(n), lil)) + geom_point() + theme_bw()

lm(lil ~ log(n), lil_df)
```





