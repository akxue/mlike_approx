# Xbeta = X %*% beta
# y[n] * X[n,], each row of X * corresponding element in y
# beta %*% colSums(y * X) %>% c # first term
loglik = (y * Xbeta + log(1 + exp(Xbeta))) %>% sum
logprior = D / 2 * log(tau) - D / 2 * log(2 * pi) -
tau / 2 * sum(u * u)
out = - loglik - logprior
return(out)
} # end psi() function
grad_psi(u_samps[1,], prior = prior)
lambda(u_samps[1,], prior = prior)
- colSums(((y - inv.logit(X %*% u_samps[1,])) %>% c) * X)
f = function(u, prior) {
# sum(log(1 + exp(prior$X %*% u)))
y   = prior$y
X   = prior$X
D   = prior$D
tau = prior$tau
Xbeta = X %*% u
# (y * Xbeta) %>% sum()
(y * Xbeta + log(1 + exp(Xbeta))) %>% sum
}
grad_logit = function(u, prior) {
grad(f, u, prior = prior)
}
f(u_samps[1,], prior)
grad_logit(u_samps[1,], prior = prior)
f = function(u, prior) {
# sum(log(1 + exp(prior$X %*% u)))
y   = prior$y
X   = prior$X
D   = prior$D
tau = prior$tau
Xbeta = X %*% u
# (y * Xbeta) %>% sum()
-(y * Xbeta + log(1 + exp(Xbeta))) %>% sum
}
f(u_samps[1,], prior)
grad_logit(u_samps[1,], prior = prior)
- colSums(((y - inv.logit(X %*% u_samps[1,])) %>% c) * X)
y * X
inv.logit(X %*% u_samps[1,]
)
-colSums(y * X)
f = function(u, prior) {
# sum(log(1 + exp(prior$X %*% u)))
y   = prior$y
X   = prior$X
D   = prior$D
tau = prior$tau
Xbeta = X %*% u
-(y * Xbeta) %>% sum()
# -(y * Xbeta + log(1 + exp(Xbeta))) %>% sum
}
grad_logit(u_samps[1,], prior = prior)
-colSums(y * X - inv.logit(X %*% u_samps[1,]) * X)
inv.logit(X %*% u_samps[1,]) * X
inv.logit(X %*% u_samps[1,])
y - inv.logit(X %*% u_samps[1,])
-colSums(y * X - c(inv.logit(X %*% u_samps[1,])) * X)
- colSums(((y - inv.logit(X %*% u_samps[1,])) %>% c) * X)
grad_logit(u_samps[1,], prior = prior)
f = function(u, prior) {
# sum(log(1 + exp(prior$X %*% u)))
y   = prior$y
X   = prior$X
D   = prior$D
tau = prior$tau
Xbeta = X %*% u
# -(y * Xbeta) %>% sum()
# -(y * Xbeta + log(1 + exp(Xbeta))) %>% sum
-log(1 + exp(Xbeta))
}
grad_logit(u_samps[1,], prior = prior)
f = function(u, prior) {
# sum(log(1 + exp(prior$X %*% u)))
y   = prior$y
X   = prior$X
D   = prior$D
tau = prior$tau
Xbeta = X %*% u
# -(y * Xbeta) %>% sum()
# -(y * Xbeta + log(1 + exp(Xbeta))) %>% sum
(-log(1 + exp(Xbeta)) * X) %>% sum
}
grad_psi(u_samps[1,], prior = prior)
f = function(u, prior) {
# sum(log(1 + exp(prior$X %*% u)))
y   = prior$y
X   = prior$X
D   = prior$D
tau = prior$tau
Xbeta = X %*% u
# -(y * Xbeta) %>% sum()
# -(y * Xbeta + log(1 + exp(Xbeta))) %>% sum
(log(1 + exp(Xbeta)) * X) %>% sum
}
grad_psi(u_samps[1,], prior = prior)
source("logistic/logistic_helper.R")
grad_logit(u_samps[1,], prior = prior)
log(1 + exp(Xbeta))
source("logistic/logistic_helper.R")
grad_logit(u_samps[1,], prior = prior)
inv.logit(X %*% u_samps[1,])
colSums(c(inv.logit(X %*% u_samps[1,])) * X)
grad_logit(u_samps[1,], prior = prior)
f = function(u, prior) {
# sum(log(1 + exp(prior$X %*% u)))
y   = prior$y
X   = prior$X
D   = prior$D
tau = prior$tau
Xbeta = X %*% u
# -(y * Xbeta) %>% sum()
# -(y * Xbeta + log(1 + exp(Xbeta))) %>% sum
(c(log(1 + exp(Xbeta))) * X) %>% sum
}
f(u_samps[1,], prior)
grad_logit(u_samps[1,], prior = prior)
colSums(c(inv.logit(X %*% u_samps[1,])) * X)
f = function(u, prior) {
# sum(log(1 + exp(prior$X %*% u)))
y   = prior$y
X   = prior$X
D   = prior$D
tau = prior$tau
Xbeta = X %*% u
# -(y * Xbeta) %>% sum()
# -(y * Xbeta + log(1 + exp(Xbeta))) %>% sum
(c(log(1 + exp(Xbeta)))) %>% sum
}
grad_logit(u_samps[1,], prior = prior)
colSums(c(inv.logit(X %*% u_samps[1,])) * X)
f = function(u, prior) {
# sum(log(1 + exp(prior$X %*% u)))
y   = prior$y
X   = prior$X
D   = prior$D
tau = prior$tau
Xbeta = X %*% u
# -(y * Xbeta) %>% sum()
# -(y * Xbeta + log(1 + exp(Xbeta))) %>% sum
(y * Xbeta + log(1 + exp(Xbeta))) %>% sum
}
grad_logit = function(u, prior) {
grad(f, u, prior = prior)
}
grad_logit(u_samps[1,], prior = prior)
colSums(c(inv.logit(X %*% u_samps[1,])) * X) + colSums(y * X)
# = -log p(y | u) - log p(u)
psi = function(u, prior) {
y   = prior$y
X   = prior$X
D   = prior$D
tau = prior$tau
Xbeta = X %*% u
# Xbeta = X %*% beta
# y[n] * X[n,], each row of X * corresponding element in y
# beta %*% colSums(y * X) %>% c # first term
loglik = (y * Xbeta - log(1 + exp(Xbeta))) %>% sum
logprior = D / 2 * log(tau) - D / 2 * log(2 * pi) -
tau / 2 * sum(u * u)
out = - loglik - logprior
return(out)
} # end psi() function
lambda(u_samps[1,], prior = prior)
grad_psi(u_samps[1,], prior = prior)
# x_test = rnorm(1000)
#
# library(microbenchmark)
microbenchmark(
"numer" = {
grad_psi(u_samps[1,], prior = prior)
},
"analytic" = {
lambda(u_samps[1,], prior = prior)
})
# analytic form of the gradient
lambda = function(u, prior) {
y   = prior$y
X   = prior$X
tau = prior$tau
grad_beta = - colSums(((y - c(inv.logit(X %*% u)))) * X) + tau * u
return(grad_beta %>% unlist %>% unname)
}
# x_test = rnorm(1000)
#
# library(microbenchmark)
microbenchmark(
"numer" = {
grad_psi(u_samps[1,], prior = prior)
},
"analytic" = {
lambda(u_samps[1,], prior = prior)
})
# x_test = rnorm(1000)
#
# library(microbenchmark)
microbenchmark(
"numer" = {
grad_psi(u_samps[1,], prior = prior)
},
"analytic" = {
lambda(u_samps[1,], prior = prior)
})
# analytic form of the gradient
lambda = function(u, prior) {
y   = prior$y
X   = prior$X
tau = prior$tau
grad_beta = - colSums(((y - c(inv.logit(X %*% u)))) * X) + tau * u
return(grad_beta)
}
# x_test = rnorm(1000)
#
# library(microbenchmark)
microbenchmark(
"numer" = {
grad_psi(u_samps[1,], prior = prior)
},
"analytic" = {
lambda(u_samps[1,], prior = prior)
})
# x_test = rnorm(1000)
#
# library(microbenchmark)
microbenchmark(
"numer" = {
grad_psi(u_samps[1,], prior = prior)
},
"analytic" = {
lambda(u_samps[1,], prior = prior)
})
lambda(u_samps[1,], prior = prior)
# analytic form of the gradient
lambda = function(u, prior) {
y   = prior$y
X   = prior$X
tau = prior$tau
grad_beta = - colSums(((y - c(inv.logit(X %*% u)))) * X) + tau * u
return(grad_beta %>% c)
}
# x_test = rnorm(1000)
#
# library(microbenchmark)
microbenchmark(
"numer" = {
grad_psi(u_samps[1,], prior = prior)
},
"analytic" = {
lambda(u_samps[1,], prior = prior)
})
source("partition/partition.R")
source("extractPartition.R")
source("hybrid_approx.R")
u_df = preprocess(u_samps, D, prior)
u_df %>% head
psi(u_samps[1,], prior)
psi(u_samps[3,], prior)
hist(psi_u)
psi_u = apply(u_samps, 1, psi, prior = prior)
hist(psi_u)
sample(10, psi_u)
psi_u %>% head
sample(psi_u, 10)
dim(u_df)
# number of MCMC samples
J = 4000
hml_approx = hml(1, D, u_df, J, prior)
u_df %>% head
u_df = preprocess(u_samps, D, prior)
u_df
u_df
u_samps
u_samps %>% head
## TODO: figure out how to specify the number of posterior samples used
bglm_fit = stan_glm(y ~ . -1, data = df,
prior = normal(location = 0, scale = 1/sqrt(tau)),
family = binomial(link = 'logit'),
refresh = 0)
summary(bglm_fit)
u_samps = bglm_fit %>% as.matrix # extract posterior samples
u_samps %>% head
u_samps = bglm_fit %>% as.data.frame() # extract posterior samples
u_samps %>% head
u_df = preprocess(u_samps, D, prior)
u_df %>% head()
lambda(u_df[1, 1:3], prior)
u_df[1, 1:3]
u_df[1, 1:3] %>% unlist %>% unname
hml_approx = hml(1, D, u_df, J, prior)
names(u_df)
traceback()
?filter
library(dplyr)
#### (1) obtain partition location of each observation
u_star = function(rpart_obj, u_df_in, partition, n_params) {
# (1.1) determine which partition each observation is grouped in
u_df = u_df_in %>% mutate(leaf_id = rpart_obj$where)
# (1.2) obtain the rows in rpart.object$frame associated with the leaf nodes
# these rows contain the fitted value for nodes that fall w/in a
# given partition
partition_id = sort(unique(rpart_obj$where)) # row id of leaf node
# number of observations in each leaf node
part_obs_tbl = table(rpart_obj$where) %>% data.frame
names(part_obs_tbl) = c("leaf_id", "n_obs")
#### (2) obtain predicted value for each of the observations
psi_hat_leaf = cbind(leaf_id = partition_id,
psi_hat = rpart_obj$frame[partition_id,]$yval) %>%
data.frame()
# append the fitted value for each row on the right as an additional column
u_df = merge(u_df, psi_hat_leaf, "leaf_id")
#### (3) compute squared residuals for each of the observations
u_df = u_df %>% mutate(dev_sq = (psi_u - psi_hat)^2)
#### (4) for each partition: compute the median of the squared residuals
# check: residual for each of the nodes should match the deviance in 'frame'
# u_df %>% group_by(leaf_id) %>% summarise(sum(dev_sq)) # matches!!!
## at this point u_df looks like: ------------------------------------------
#
# | leaf_id |   u1   |   u2   | ... |   up  |  psi_u  |  psi_hat |  dev_sq
# |------------------------------------------------------------------------
# |       4 |  0.423 | -4.584 | ... |   up  | -10.436 |  -6.522  |  15.315
# |       4 | -0.425 | -4.455 | ... |   up  | -8.1148 |  -6.522  |  2.5353
# |     ... |    ... |    ... | ... |   up  |    ...  |     ...  |    ...
#
## -------------------------------------------------------------------------
# (4.1) for each partition: sort the rows by squared residual values
n_partitions = length(partition_id)
# n_params = 2 # DONE: this should be passed into the function
# initialize data.frame to store the representative points of each partition
# (n_partitions x n_params)
# DONE: column names should match the convention of 'u1, u2, ... '
# extra column for the lead id
u_star_df = matrix(NA, n_partitions, n_params + 1) %>% data.frame()
for (k in 1:n_partitions) {
# number of observations in partition k
n_obs = part_obs_tbl$n_obs[k]
# subset out observations in partition k, sort on dev_sq column
sorted_partition = u_df %>% dplyr::filter(leaf_id == partition_id[k]) %>%
arrange(dev_sq)
# (4.2) for each partition: save the row whose squared residual value is
# the median; this point is that partitions's "representative point"
# extract row corresponding to the median
med_row = floor(n_obs / 2)
part_k_med = sorted_partition[med_row,]
part_k_med = sorted_partition[1,]
# extract the 'representative point' of partition k -- this will be a
# p-dim vector
u_vec_k = (part_k_med %>%
dplyr::select(u1:psi_u))[, -(n_params + 1)] %>%
as.matrix() %>% c()
u_star_df[k,] = c(u_vec_k, part_k_med$leaf_id)
} # end of loop extracting representative points
# 1/14 -- generalizing this to D many parameters -- DONE
u_star_names = character(n_params + 1)
for (d in 1:n_params) {
u_star_names[d] = paste(names(u_df_in)[d], '_star', sep = '')
}
u_star_names[n_params + 1] = "leaf_id"
names(u_star_df) = u_star_names
# names(u_star_df) = c("u1_star", "u2_star", "leaf_id")
## merge with the boundary of each of the partitions
u_df_full = merge(u_star_df, partition, by = 'leaf_id')
# append the number of observations for each leaf node to the right
# this is later used to determine the type of approximation to use
u_df_full = merge(u_df_full, part_obs_tbl, by = 'leaf_id')
return(u_df_full)
} # end u_star() function ------------------------------------------------------
hml_approx = hml(1, D, u_df, J, prior)
library(rstan)
library(rstudioapi) # running  RStan in parallel via Rstudio
# DELL_PATH = "C:/Users/chuu/mlike_approx"
LEN_PATH  = "C:/Users/ericc/mlike_approx"
# path for lenovo
setwd(LEN_PATH)
# path for dell
# setwd(DELL_PATH)
source("partition/partition.R")
source("extractPartition.R")
source("hybrid_approx.R")
source("mvn_ig/mvn_ig_helper.R") # load this LAST to overwrite def preprocess()
# STAN sampler settings --------------------------------------------------------
J         = 500          # number of MC samples per approximation
N_approx  = 1            # number of approximations
burn_in   = 2000         # number of burn in draws
n_chains  = 4            # number of markov chains to run
stan_seed = 123          # seed
J_iter = 1 / n_chains * N_approx * J + burn_in
K_sims = 1               # num of simulations to run FOR EACH N in N_vec
# ------------------------------------------------------------------------------
# D_vec = c(3, 5, 7, 10)
D_vec = c(3)
LIL_d = vector("list", length = length(D_vec))
# N_vec = c(50, 60, 70, 100, 110, 125, 150, 200, 225, 250, 300)
N_vec = c(200) # for testing -- comment this line to perform ext. analysis
LIL_const  = matrix(NA, N_approx, length(N_vec))
LIL_hybrid = matrix(NA, N_approx, length(N_vec))
LIL_taylor  = matrix(NA, N_approx, length(N_vec))
set.seed(123)
d_i = 1
D = D_vec[d_i]
print(paste("Performing LIL approximation for D = ", D, sep = ''))
# priors, true parameter values --------------------------------------------
# D       = 3                # dimension of paramter
p       = D - 1            # dimension of beta
mu_beta = rep(0, p)        # prior mean for beta
V_beta  = diag(1, p)       # scaled precision matrix for betaV_beta
a_0     = 2 / 2            # shape param for sigmasq
b_0     = 1 / 2            # scale param
beta    = sample(-10:10, p, replace = T)
sigmasq = 4                # true variance (1 x 1)
I_p = diag(1, p)       # (p x p) identity matrix
# --------------------------------------------------------------------------
LIL_N = numeric(length(N_vec))      # store the LIL for each N
LIL_N_hat = numeric(length(N_vec))  # store LIL approximations for N
i = 1
N = N_vec[i]
I_N = diag(1, N)       # (N x N) identity matrix
LIL_N_k = numeric(K_sims)     # store the true LIL for K_sims results
LIL_N_k_hat = numeric(K_sims) # store the approx for K_sims results
print(paste('iter = ', i, ' -- calculating LIL for N = ', N,
' (', K_sims, ' sims)', sep = ''))
k = 1
## simulate N data points + sample from posterior ------------------
X = matrix(rnorm(N * p), N, p) # (N x p) design matrix
eps = rnorm(N, mean = 0, sd = sqrt(sigmasq))
y = X %*% beta + eps # (N x 1) response vector
## compute posterior parameters ------------------------------------
V_beta_inv = solve(V_beta)
V_star_inv = t(X) %*% X + V_beta_inv
V_star  = solve(V_star_inv)                                # (p x p)
mu_star = V_star %*% (t(X) %*% y + V_beta_inv %*% mu_beta) # (p x 1)
a_n =  a_0 + N / 2
b_n =  c(b_0 + 0.5 * (t(y) %*% y +
t(mu_beta) %*% V_beta_inv %*% mu_beta -
t(mu_star) %*% V_star_inv %*% mu_star))
# compute MLE estimates for mean, variance of the regression model
ybar = X %*% mu_star
sigmasq_mle = 1 / N * sum((y - ybar)^2)
# create prior, posterior objects
prior = list(V_beta = V_beta,
mu_beta = mu_beta,
a_0 = a_0,
b_0 = b_0,
y = y, X = X,
V_beta_inv = V_beta_inv)
# store posterior parameters
post  = list(V_star  =  V_star,
mu_star =  mu_star,
a_n     =  a_n,
b_n     =  b_n,
V_star_inv = V_star_inv)
# subtract log max likelihood to stabilize approximation
LIL_N_k[k] = lil(y, X, prior, post) #-
## form the approximation
post_dat = list(p = p,
a_n = a_n, b_n = b_n,
mu_star = c(mu_star), V_star = V_star)
mvnig_fit = stan(file   = 'mvn_ig/mvn_ig_sampler.stan',
data    = post_dat,
iter    = J_iter,
warmup  = burn_in,
chains  = n_chains,
refresh = 0) # should give us J * N_approx draws
# use special preprocess b/c we call psi_true()
u_df = preprocess(mvnig_fit, D, post, prior)
u_df %>% head
# subtract log max likelihood to stabilize approximation
LIL_N_k_hat[k] = mean(approx_lil(N_approx, D, u_df, J, prior)) #-
hml_approx = hml(1, D, u_df, J, prior)
install.packages("tibble")
install.packages("tibble")
library(dplyr)
library(tibble)
library(dplyr)
hml_approx = hml(1, D, u_df, J, prior)
hml_approx = hml(1, D, u_df, J, prior)
u_df %>% dplyr::select(u1:psi_u) %>% head
rlang::last_error()
u_df %>% dplyr::select(u1)
u_df %>% dplyr::select("u1")
u_df %>% select("u1")
u_df %>% select(u1)
u_df %>% dplyr::select(u1)
hml_approx = hml(1, D, u_df, J, prior)
n
n
n
n
n
n
n
n
part_k_med
1 + 1
u_df %>% head
u_df %>% select(u1)
u_df %>% dplyr::select(u1)
head(u_df)
u_df %>% select(u1)
u_df %>% dplyr::select(u1)
head(u_df)
u_df %>% dplyr::select(u1)
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("vctrs")
install.packages("vctrs")
