J = 200      # number of MCMC samples to use per approximation
# read in posterior samples
u_samps = read.csv("RRR/u_df_rrr.csv", header = FALSE)
dim(u_samps)
## evaluate psi() function for each of the posterior samples (row-wise)
u_df = preprocess(u_samps, d, param_list) # J x (d + 1)
u = u_df[1, 1:d] %>% unlist %>% unname
head(u_f)
head(u_df)
J = nrow(u_df) # number of MCMC samples to use in the approximation
hml_approx = hml(1, d, u_df, J, param_list)
hml_approx$const_vec
hml_approx$taylor_vec
hml_approx$hybrid_vec
hml_approx$n_taylor
hml_approx$verbose_partition
hml_approx$taylor_approx
hml_approx$ck_2
ind = 20
hml_approx$ck_3[ind]
upper = hml_approx$partition$u20_ub[6]
lower = hml_approx$partition$u20_lb[6]
l_k_d = hml_approx$lambda[6,ind]
- l_k_d * upper + log(- 1 / l_k_d * (1 - exp(-l_k_d * lower + l_k_d * upper)))
exp(-l_k_d * lower + l_k_d * upper) # overflowing
exp(-l_k_d * lower) * exp(l_k_d * upper)
-l_k_d * lower + l_k_d * upper
log(1 - exp(-l_k_d * lower + l_k_d * upper))
exp(-2) - exp(-1)
library(TruncatedNormal)
library(tmg)
library(mvtnorm)
LEN_PATH  = "C:/Users/ericc/mlike_approx"
setwd(LEN_PATH)
source("partition/partition.R")
source("extractPartition.R")
source("hybrid_approx.R")
source("truncate/regTN_helper.R")
D = 100
N = 200
I_D = diag(1, D)
# prior mean
mu_0 = rep(0, D)
tau     = 1 / 4          # precision: inverse of variance
sigmasq = 4              # true variance (1 x 1)
# true value of beta
set.seed(1)
beta = sample(0:10, D, replace = T)
beta = runif(D)
beta = c(runif(D-1, 0, 1), 0)
J        = 100         # number of MC samples
B        = 10          # number of batch estimates
N_approx = 1           # number of estimates to compute per iteration b
# ------------------------------------------------------------------------------
set.seed(1)
K_sims    = 100                              # num of sims to run for each N
N_vec_log = seq(5, 10, by = 0.25)            # sample size grid unif in log
N_vec     = floor(exp(N_vec_log)) %>% unique # sample size to generate data
LIL_N = numeric(length(N_vec))      # store the LIL for each N
LIL_N_k_hat = matrix(0, length(N_vec), K_sims)
library(TruncatedNormal)
library(tmg)
library(mvtnorm)
LEN_PATH  = "C:/Users/ericc/mlike_approx"
setwd(LEN_PATH)
source("partition/partition.R")
source("extractPartition.R")
source("hybrid_approx.R")
source("truncate/regTN_helper.R")
D = 100
N = 200
I_D = diag(1, D)
# prior mean
mu_0 = rep(0, D)
tau     = 1 / 4          # precision: inverse of variance
sigmasq = 4              # true variance (1 x 1)
# true value of beta
set.seed(1)
beta = sample(0:10, D, replace = T)
beta = runif(D)
beta = c(runif(D-1, 0, 1), 0)
J        = 100         # number of MC samples
B        = 10          # number of batch estimates
N_approx = 1           # number of estimates to compute per iteration b
# ------------------------------------------------------------------------------
set.seed(1)
K_sims    = 100                              # num of sims to run for each N
N_vec_log = seq(5, 10, by = 0.25)            # sample size grid unif in log
N_vec     = floor(exp(N_vec_log)) %>% unique # sample size to generate data
LIL_N = numeric(length(N_vec))      # store the LIL for each N
LIL_N_k_hat = matrix(0, length(N_vec), K_sims)
i = 1; k = 1;
N = N_vec[i]
# generate data --------------------------------------------------------
X   = matrix(rnorm(N * D), N, D)               # (N x D) design matrix
eps = rnorm(N, mean = 0, sd = sqrt(sigmasq))   # (N x 1) errors vector
y   = X %*% beta + eps                         # (N x 1) response vector
# compute posterior parameters -----------------------------------------
Q_beta     =  1 / sigmasq * (t(X) %*% X + tau * diag(1, D))
Q_beta_inv =  solve(Q_beta)
b          =  1 / sigmasq * t(X) %*% y
mu_beta    =  Q_beta_inv %*% b
loglik_max = - 0.5 * N * log(2 * pi * sigmasq) -
1 / (2 * sigmasq) * sum((y - X %*% mu_beta)^2)
# create prior, post objects to be passed into the hml algorithm
prior = list(y = y, X = X, sigmasq = sigmasq, tau = tau, N = N, D = D,
Q_beta = Q_beta, b = b)
post = list(Q_beta = Q_beta, Q_beta_inv = Q_beta_inv,
mu_beta = mu_beta, b = b)
lil_0 = -0.5 * N * log(2 * pi) - 0.5 * (N + D) * log(sigmasq) +
0.5 * D * log(tau) - 0.5 * log_det(Q_beta) -
1 / (2 * sigmasq) * sum(y^2) + 0.5 * sum(b * mu_beta) + D * log(2)
lil_0 = -0.5 * N * log(2 * pi) - 0.5 * (N + D) * log(sigmasq) +
0.5 * D * log(tau) - 0.5 * log_det(Q_beta) -
1 / (2 * sigmasq) * sum(y^2) + 0.5 * sum(b * mu_beta)
lil_0 +
log(TruncatedNormal::pmvnorm(mu_beta, Q_beta_inv, lb = rep(0, D),
ub = rep(Inf, D))[1]) - loglik_max
-0.5 * N * log(2 * pi) - 0.5 * (N + D) * log(sigmasq) +
0.5 * D * log(tau) - 0.5 * log_det(Q_beta) -
1 / (2 * sigmasq) * sum(y^2) + 0.5 * sum(b * mu_beta) + D * log(2)
lil_0 = -0.5 * N * log(2 * pi) - 0.5 * (N + D) * log(sigmasq) +
0.5 * D * log(tau) - 0.5 * log_det(Q_beta) -
1 / (2 * sigmasq) * sum(y^2) + 0.5 * sum(b * mu_beta) + D * log(2)
lil_0 +
log(TruncatedNormal::pmvnorm(mu_beta, Q_beta_inv, lb = rep(0, D),
ub = rep(Inf, D))[1]) - loglik_max
+ D * log(2)
D = 2
N = 200
I_D = diag(1, D)
# prior mean
mu_0 = rep(0, D)
tau     = 1 / 4          # precision: inverse of variance
sigmasq = 4              # true variance (1 x 1)
# true value of beta
set.seed(1)
beta = sample(0:10, D, replace = T)
beta = runif(D)
beta = c(runif(D-1, 0, 1), 0)
J        = 100         # number of MC samples
B        = 10          # number of batch estimates
N_approx = 1           # number of estimates to compute per iteration b
# ------------------------------------------------------------------------------
set.seed(1)
K_sims    = 100                              # num of sims to run for each N
N_vec_log = seq(5, 10, by = 0.25)            # sample size grid unif in log
N_vec     = floor(exp(N_vec_log)) %>% unique # sample size to generate data
LIL_N = numeric(length(N_vec))      # store the LIL for each N
LIL_N_k_hat = matrix(0, length(N_vec), K_sims)
for (i in 1:length(N_vec)) {
N = N_vec[i]
# print(paste("N = ", N, sep = ''))
LIL_N_k = numeric(K_sims)     # store the true LIL for K_sims
# for each N, each of the K_sims are stored row-wise
for (k in 1:K_sims) {
# generate data --------------------------------------------------------
X   = matrix(rnorm(N * D), N, D)               # (N x D) design matrix
eps = rnorm(N, mean = 0, sd = sqrt(sigmasq))   # (N x 1) errors vector
y   = X %*% beta + eps                         # (N x 1) response vector
# compute posterior parameters -----------------------------------------
Q_beta     =  1 / sigmasq * (t(X) %*% X + tau * diag(1, D))
Q_beta_inv =  solve(Q_beta)
b          =  1 / sigmasq * t(X) %*% y
mu_beta    =  Q_beta_inv %*% b
loglik_max = - 0.5 * N * log(2 * pi * sigmasq) -
1 / (2 * sigmasq) * sum((y - X %*% mu_beta)^2)
# create prior, post objects to be passed into the hml algorithm
prior = list(y = y, X = X, sigmasq = sigmasq, tau = tau, N = N, D = D,
Q_beta = Q_beta, b = b)
post = list(Q_beta = Q_beta, Q_beta_inv = Q_beta_inv,
mu_beta = mu_beta, b = b)
# ----------------------------------------------------------------------
# compute true LIL first -----------------------------------------------
# TODO: uncomment the following two calculations if we want to include
# those in the final figure, but it might be enough to just look at
# the hybrid approximations
lil_0 = -0.5 * N * log(2 * pi) - 0.5 * (N + D) * log(sigmasq) +
0.5 * D * log(tau) - 0.5 * log_det(Q_beta) -
1 / (2 * sigmasq) * sum(y^2) + 0.5 * sum(b * mu_beta) + D * log(2)
LIL_N_k[k] = lil_0 +
log(TruncatedNormal::pmvnorm(mu_beta, Q_beta_inv, lb = rep(0, D),
ub = rep(Inf, D))[1]) - loglik_max
# ----------------------------------------------------------------------
# sample from posterior, u
samples = data.frame(rtmvnorm(J, c(mu_beta), Q_beta_inv,
rep(0, D), rep(Inf, D)))
# evaluate psi(u)
u_df = preprocess(samples, D, prior)
# compute approximation
hml_approx = hml(N_approx, D, u_df, J, prior)
LIL_N_k_hat[i, k] = hml_approx$hybrid_vec - loglik_max
# subtract the loglikelihood
} # end of k-sims
LIL_N[i] = mean(LIL_N_k)
print(paste("iter ", i, "/", length(N_vec), ": ",
"approx LIL for N = ", N, " -- LIL = ",
round(mean(LIL_N_k_hat[i, ]), 2),
" (", round(LIL_N[i], 2), ")", sep = ''))
}
source("misc.R")
for (i in 1:length(N_vec)) {
N = N_vec[i]
# print(paste("N = ", N, sep = ''))
LIL_N_k = numeric(K_sims)     # store the true LIL for K_sims
# for each N, each of the K_sims are stored row-wise
for (k in 1:K_sims) {
# generate data --------------------------------------------------------
X   = matrix(rnorm(N * D), N, D)               # (N x D) design matrix
eps = rnorm(N, mean = 0, sd = sqrt(sigmasq))   # (N x 1) errors vector
y   = X %*% beta + eps                         # (N x 1) response vector
# compute posterior parameters -----------------------------------------
Q_beta     =  1 / sigmasq * (t(X) %*% X + tau * diag(1, D))
Q_beta_inv =  solve(Q_beta)
b          =  1 / sigmasq * t(X) %*% y
mu_beta    =  Q_beta_inv %*% b
loglik_max = - 0.5 * N * log(2 * pi * sigmasq) -
1 / (2 * sigmasq) * sum((y - X %*% mu_beta)^2)
# create prior, post objects to be passed into the hml algorithm
prior = list(y = y, X = X, sigmasq = sigmasq, tau = tau, N = N, D = D,
Q_beta = Q_beta, b = b)
post = list(Q_beta = Q_beta, Q_beta_inv = Q_beta_inv,
mu_beta = mu_beta, b = b)
# ----------------------------------------------------------------------
# compute true LIL first -----------------------------------------------
# TODO: uncomment the following two calculations if we want to include
# those in the final figure, but it might be enough to just look at
# the hybrid approximations
lil_0 = -0.5 * N * log(2 * pi) - 0.5 * (N + D) * log(sigmasq) +
0.5 * D * log(tau) - 0.5 * log_det(Q_beta) -
1 / (2 * sigmasq) * sum(y^2) + 0.5 * sum(b * mu_beta) + D * log(2)
LIL_N_k[k] = lil_0 +
log(TruncatedNormal::pmvnorm(mu_beta, Q_beta_inv, lb = rep(0, D),
ub = rep(Inf, D))[1]) - loglik_max
# ----------------------------------------------------------------------
# sample from posterior, u
samples = data.frame(rtmvnorm(J, c(mu_beta), Q_beta_inv,
rep(0, D), rep(Inf, D)))
# evaluate psi(u)
u_df = preprocess(samples, D, prior)
# compute approximation
hml_approx = hml(N_approx, D, u_df, J, prior)
LIL_N_k_hat[i, k] = hml_approx$hybrid_vec - loglik_max
# subtract the loglikelihood
} # end of k-sims
LIL_N[i] = mean(LIL_N_k)
print(paste("iter ", i, "/", length(N_vec), ": ",
"approx LIL for N = ", N, " -- LIL = ",
round(mean(LIL_N_k_hat[i, ]), 2),
" (", round(LIL_N[i], 2), ")", sep = ''))
}
misc.R
source("hybrid_approx.R")
library(rstan)
library(rstudioapi) # running  RStan in parallel via Rstudio
library(reshape2)
rstan_options(auto_write = TRUE)
# DELL_PATH = "C:/Users/chuu/mlike_approx"
LEN_PATH  = "C:/Users/ericc/mlike_approx"
# path for lenovo
setwd(LEN_PATH)
source("partition/partition.R")
source("extractPartition.R")
source("hybrid_approx.R")
source("mvn_ig/mvn_ig_helper.R") # load this LAST to overwrite def preprocess()
N_approx  = 1            # number of approximations
burn_in   = 2000         # number of burn in draws
n_chains  = 4            # number of markov chains to run
stan_seed = 123          # seed
K_sims    = 1            # num of simulations to run FOR EACH N in N_vec
D_vec   = c(3)
J_samps = c(40)
N = 1000 # fix this value for now
set.seed(123)
B = 20
library(TruncatedNormal)
library(tmg)
library(mvtnorm)
LEN_PATH  = "C:/Users/ericc/mlike_approx"
setwd(LEN_PATH)
source("partition/partition.R")
source("extractPartition.R")
source("hybrid_approx.R")
source("truncate/regTN_helper.R")
source("misc.R")
D = 2
N = 200
I_D = diag(1, D)
# prior mean
mu_0 = rep(0, D)
tau     = 1 / 4          # precision: inverse of variance
sigmasq = 4              # true variance (1 x 1)
# true value of beta
set.seed(1)
beta = sample(0:10, D, replace = T)
beta = runif(D)
beta = c(runif(D-1, 0, 1), 0)
J        = 100         # number of MC samples
B        = 10          # number of batch estimates
N_approx = 1           # number of estimates to compute per iteration b
set.seed(1)
K_sims    = 100                              # num of sims to run for each N
N_vec_log = seq(5, 10, by = 0.25)            # sample size grid unif in log
N_vec     = floor(exp(N_vec_log)) %>% unique # sample size to generate data
LIL_N = numeric(length(N_vec))      # store the LIL for each N
LIL_N_k_hat = matrix(0, length(N_vec), K_sims)
for (i in 1:length(N_vec)) {
N = N_vec[i]
# print(paste("N = ", N, sep = ''))
LIL_N_k = numeric(K_sims)     # store the true LIL for K_sims
# for each N, each of the K_sims are stored row-wise
for (k in 1:K_sims) {
# generate data --------------------------------------------------------
X   = matrix(rnorm(N * D), N, D)               # (N x D) design matrix
eps = rnorm(N, mean = 0, sd = sqrt(sigmasq))   # (N x 1) errors vector
y   = X %*% beta + eps                         # (N x 1) response vector
# compute posterior parameters -----------------------------------------
Q_beta     =  1 / sigmasq * (t(X) %*% X + tau * diag(1, D))
Q_beta_inv =  solve(Q_beta)
b          =  1 / sigmasq * t(X) %*% y
mu_beta    =  Q_beta_inv %*% b
loglik_max = - 0.5 * N * log(2 * pi * sigmasq) -
1 / (2 * sigmasq) * sum((y - X %*% mu_beta)^2)
# create prior, post objects to be passed into the hml algorithm
prior = list(y = y, X = X, sigmasq = sigmasq, tau = tau, N = N, D = D,
Q_beta = Q_beta, b = b)
post = list(Q_beta = Q_beta, Q_beta_inv = Q_beta_inv,
mu_beta = mu_beta, b = b)
# ----------------------------------------------------------------------
# compute true LIL first -----------------------------------------------
# TODO: uncomment the following two calculations if we want to include
# those in the final figure, but it might be enough to just look at
# the hybrid approximations
lil_0 = -0.5 * N * log(2 * pi) - 0.5 * (N + D) * log(sigmasq) +
0.5 * D * log(tau) - 0.5 * log_det(Q_beta) -
1 / (2 * sigmasq) * sum(y^2) + 0.5 * sum(b * mu_beta) + D * log(2)
LIL_N_k[k] = lil_0 +
log(TruncatedNormal::pmvnorm(mu_beta, Q_beta_inv, lb = rep(0, D),
ub = rep(Inf, D))[1]) - loglik_max
# ----------------------------------------------------------------------
# sample from posterior, u
samples = data.frame(rtmvnorm(J, c(mu_beta), Q_beta_inv,
rep(0, D), rep(Inf, D)))
# evaluate psi(u)
u_df = preprocess(samples, D, prior)
# compute approximation
hml_approx = hml(N_approx, D, u_df, J, prior)
LIL_N_k_hat[i, k] = hml_approx$hybrid_vec - loglik_max
# subtract the loglikelihood
} # end of k-sims
LIL_N[i] = mean(LIL_N_k)
print(paste("iter ", i, "/", length(N_vec), ": ",
"approx LIL for N = ", N, " -- LIL = ",
round(mean(LIL_N_k_hat[i, ]), 2),
" (", round(LIL_N[i], 2), ")", sep = ''))
}
library(reshape2)
library(ggpmisc)
formula1 = y ~ x
lil_0   = LIL_N                  # length(N_vec) x 1
lil_hyb = rowMeans(LIL_N_k_hat)  # length(N_vec) x 1
log_N   = log(N_vec)             # length(N_vec) x 1
LIL_df = data.frame(LIL_N = lil_0, LIL_hat = lil_hyb, log_N = log(N_vec))
LIL_df_long = melt(LIL_df, id.vars = "log_N")
head(LIL_df_long)
ggplot(LIL_df_long, aes(x = log_N, y = value,
color = as.factor(variable))) + geom_point(size = 1.3) +
geom_smooth(method = lm, se = F, formula = formula1) +
labs(x = "log(n)", y = "log(Z)",
title = "True (Red), Hybrid (Blue)") +
stat_poly_eq(aes(label = paste(..eq.label.., sep = "~~~")),
label.x.npc = "right", label.y.npc = "top",
eq.with.lhs = "logZ~`=`~",
eq.x.rhs = "~logN",
formula = formula1, parse = TRUE, size = 8) +
theme_bw(base_size = 16) +
theme(legend.position = "none")
##  RRR_sample.R
##  read in the posterior samples written to .csv from MATLAB script
library(dplyr)
library(mvtnorm)
# path for lenovo
LEN_PATH  = "C:/Users/ericc/mlike_approx"
setwd(LEN_PATH)
# files must be loaded in this order, since *_helper.R file will sometimes
# overwrite a functino defined in hybrid_approx.R depending on the example
source("partition/partition.R")         # load partition extraction functions
source("hybrid_approx.R")               # load main algorithm functions
source("RRR/RRR_helper.R")              # load psi(), lambda(), preprocess()
source('extractPartition.R')            # load extractPartition() function
## read in true parameters generated/computed via MATLAB script ----------------
##
# covariates
# read in design matrix
X = read.csv("RRR/X.csv", header = FALSE) %>% as.matrix
# read in parameters A, B
A = read.csv("RRR/A.csv", header = FALSE) %>% as.matrix
B = read.csv("RRR/B.csv", header = FALSE) %>% as.matrix
# read in error: eps
eps = read.csv("RRR/eps.csv", header = FALSE) %>% as.matrix
# compute: Y = X * A * B' + eps
Y = X %*% A %*% t(B) + eps
# compute C = A * B'
C = A %*% t(B)
##
## -----------------------------------------------------------------------------
# various dimension settings
# dimensions can be computed from the dimensions of the input above
p = ncol(X)    # number of columns in X
q = ncol(Y)    # number of columns in Y
r = ncol(A)    # number of columns in B and A
n = nrow(X)    # number of rows in X and Y
# prior parameters
sig2 = 10^(-2);  # fixed for now
del  = 10^(-2);
# dimension of the parameter, u (drawn from posterior)
d = r * p + r * q
# various identity matrices
I_p = diag(1, p)
I_q = diag(1, q)
I_n = diag(1, n)
I_r = diag(1, r)
# store parameters in param object
param_list = list(p = p, q = q, r = r, n = n, d = d,   # dimensions variables
Y = Y, X = X,                        # response, design matrix
sig2 = sig2, del = del)              # prior params
burn = 500   # number of samples to discard
J = 200      # number of MCMC samples to use per approximation
# read in posterior samples
u_samps = read.csv("RRR/u_df_rrr.csv", header = FALSE)
dim(u_samps)
### can stick the following directly into preprocess function
# u_samps = u_samps[-c(1:burn),]
# u_samps_sub = u_samps[1:J,]    # (J x d) -- MCMC samples stored row-wise
## evaluate psi() function for each of the posterior samples (row-wise)
u_df = preprocess(u_samps, d, param_list) # J x (d + 1)
u = u_df[1, 1:d] %>% unlist %>% unname
lambda(u, param_list)
J = nrow(u_df) # number of MCMC samples to use in the approximation
hml_approx = hml(1, d, u_df, J, param_list)
ind = 20
hml_approx$ck_3[ind]
upper = hml_approx$partition$u20_ub[6]
lower = hml_approx$partition$u20_lb[6]
l_k_d = hml_approx$lambda[6,ind]
dim(hml_approx$lambda) # 6 partitions x 28 dim parameter
exp(-l_k_d * upper + l_k_d * lower )
exp(-l_k_d * upper + l_k_d * lower)
-l_k_d * upper + l_k_d * lower
l_k_d
library(VGAM) # log1mexp(x)
# overflowing for exp(x), x > 700
log1mexp(l_k_d * lower - l_k_d * upper)
？log1mexp
？log1mexp
?log1mexp
# overflowing for exp(x), x > 700
log1mexp(l_k_d * upper - l_k_d * lower)
l_k_d * upper - l_k_d * lower
log1mexp(l_k_d * upper - l_k_d * lower)
- l_k_d * lower - log(l_k_d) + log1mexp(l_k_d * upper - l_k_d * lower)
hml_approx$ck_3
ind = 2
hml_approx$ck_3[2]
upper = hml_approx$partition$u2_ub[6]
lower = hml_approx$partition$u2_lb[6]
l_k_d = hml_approx$lambda[6,ind]
- l_k_d * upper + log(- 1 / l_k_d * (1 - exp(-l_k_d * lower + l_k_d * upper)))
- l_k_d * lower - log(l_k_d) + log1mexp(l_k_d * upper - l_k_d * lower)
hml_approx$lambda
hml_approx$lambda[6,]
ind = 28
hml_approx$ck_3[ind]
upper = hml_approx$partition$u28_ub[6]
lower = hml_approx$partition$u28_lb[6]
l_k_d = hml_approx$lambda[6,ind]
- l_k_d * upper + log(- 1 / l_k_d * (1 - exp(-l_k_d * lower + l_k_d * upper)))
- l_k_d * lower - log(l_k_d) + log1mexp(l_k_d * upper - l_k_d * lower)
x = -0.5
log(-1/x)
-log(x)
-log(-x)
hml_approx$lambda[6,]
ind = 3
hml_approx$ck_3[ind]
l_k_d = hml_approx$lambda[6,ind]
l_k_d
- l_k_d * upper + log(- 1 / l_k_d * (1 - exp(-l_k_d * lower + l_k_d * upper)))
hml_approx$ck_3
upper = hml_approx$partition$u3_ub[6]
lower = hml_approx$partition$u3_lb[6]
l_k_d = hml_approx$lambda[6,ind]
- l_k_d * upper + log(- 1 / l_k_d * (1 - exp(-l_k_d * lower + l_k_d * upper)))
# for lambda > 0
- l_k_d * lower - log(l_k_d) + log1mexp(l_k_d * upper - l_k_d * lower)
# for lambda < 0
-log(-l_k_d) - l_k_d * b + log1mexp(l_k_d * lower - l_k_d * upper)
# for lambda < 0
-log(-l_k_d) - l_k_d * upper + log1mexp(l_k_d * lower - l_k_d * upper)
ind = 18
hml_approx$ck_3[ind]
upper = hml_approx$partition$u18_ub[6]
lower = hml_approx$partition$u18_lb[6]
l_k_d = hml_approx$lambda[6,ind]
(l_k_d = hml_approx$lambda[6,ind])
- l_k_d * upper + log(- 1 / l_k_d * (1 - exp(-l_k_d * lower + l_k_d * upper)))
hml_approx$ck_3[ind]
- l_k_d * upper + log(- 1 / l_k_d * (1 - exp(-l_k_d * lower + l_k_d * upper)))
# for lambda > 0
- l_k_d * lower - log(l_k_d) + log1mexp(l_k_d * upper - l_k_d * lower)
# for lambda < 0
-log(-l_k_d) - l_k_d * upper + log1mexp(l_k_d * lower - l_k_d * upper)
